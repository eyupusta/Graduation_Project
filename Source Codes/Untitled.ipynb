{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm \n",
    "from sklearn.utils import shuffle \n",
    "from collections import Counter \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arff(filename):\n",
    "\ttry:\n",
    "\t\tfile = open(filename)\n",
    "\texcept:\n",
    "\t\tprint('file could not found or opened, try with file location')\n",
    "\tcolumns = []\n",
    "\tdata = []\n",
    "\n",
    "\tfor line in file:\n",
    "\t\tif line.startswith('@attribute'):\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\ttemp = line.split()\n",
    "\t\t\tcolumns.append(temp[1])\n",
    "\t\telif line.startswith('@') or line.startswith('\\n'):\n",
    "\t\t\tcontinue\n",
    "\t\telse:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\ttemp = line.split(',')\n",
    "\t\t\tdata.append(temp)\n",
    "\n",
    "\treturn data,columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sizes = [0.15, 0.2, 0.25, 0.3, 0.35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, columns = parse_arff('./Training Dataset.arff')\n",
    "df = pd.DataFrame(data, columns = columns, dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "C = [1, 2, 3, 4, 5] # regularization parameter\n",
    "degree = [1, 2, 3, 4, 5, 6] # polinomial degree\n",
    "gamma = ['scale', 'auto']\n",
    "decision_function_shape = ['ovo', 'ovr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = ['hinge', 'squared_hinge']\n",
    "penalty = ['l2']\n",
    "C = [1, 2, 3, 4, 5] # regularization parameter\n",
    "multi_class = ['ovr', 'crammer_singer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ['uniform', 'distance']\n",
    "algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "metric = ['euclidean', 'manhattan', 'chebyshev', 'minkowski']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = [None, 'auto', 'sqrt', 'log2']\n",
    "criterion = ['gini', 'entropy']\n",
    "splitter = ['best', 'random']\n",
    "bootstrap  = [True, False]\n",
    "class_weight = [None, 'balanced', 'balanced_subsample']\n",
    "warm_start = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0,1.,2.,3.,4.,5.,7.,9.,11.]\n",
    "binarize = [0,1.,2.,3.,4.,5.,7.,9.,11.] #BernoulliNB\n",
    "fit_prior = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_models = []\n",
    "lsvc = []\n",
    "knn = []\n",
    "bnbmodels = []\n",
    "randomforest = []\n",
    "dtree = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-af6d818e5290>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mgam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mdecision\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdecision_function_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                         \u001b[0msvc_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecision_function_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecision\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;31m###############################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_vectors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_support\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual_coef_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_probA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m             self._probB, self.fit_status_ = libsvm.fit(\n\u001b[0m\u001b[0;32m    269\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m                 \u001b[0msvm_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for size in test_sizes:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df[df.columns[:-1]],df[df.columns[-1]], test_size = size)\n",
    "    x_train = np.array(x_train).astype(np.int8)\n",
    "    y_train = np.array(y_train).astype(np.int8)\n",
    "    x_test = np.array(x_test).astype(np.int8)\n",
    "    y_test = np.array(y_test).astype(np.int8)\n",
    "    y_train = np.where(y_train == -1, 0, y_train)\n",
    "    y_test = np.where(y_test == -1, 0, y_test)\n",
    "    ###############################################\n",
    "    for kernel in kernels:\n",
    "        for c in C:\n",
    "            for deg in degree:\n",
    "                for gam in gamma:\n",
    "                    for decision in decision_function_shape:\n",
    "                        svc_models.append(SVC(kernel = kernel, C = c, degree = deg, gamma = gam, decision_function_shape=decision).fit(x_train, y_train))\n",
    "    ###############################################\n",
    "    for loss in losses:\n",
    "        for c in C:\n",
    "            for multi in multi_class:\n",
    "                lsvc.append(LinearSVC(loss = loss, penalty = 'l2', C = c, multi_class = multi).fit(x_train, y_train))\n",
    "    ###############################################\n",
    "    for k in range(1, 10):\n",
    "        for weight in weights:\n",
    "            for algo in algorithm:\n",
    "                for m in metric:\n",
    "                    knn.append(KNeighborsClassifier(n_neighbors=k, weights = weight, algorithm = algo, metric=m).fit(x_train, y_train))\n",
    "    ###############################################\n",
    "    for a in alpha:\n",
    "        for b in binarize:\n",
    "            for fit in fit_prior:\n",
    "                bnbmodels.append(BernoulliNB(alpha = a, binarize = b, fit_prior = fit).fit(x_train, y_train))\n",
    "    ###############################################    \n",
    "    for weight in class_weight:\n",
    "        for maxx in max_features:\n",
    "            for cri in criterion:\n",
    "                for s in warm_start:\n",
    "                    randomforest.append(RandomForestClassifier(warm_start = s, class_weight=weight, max_features = maxx, criterion = cri).fit(x_train, y_train))\n",
    "    ###############################################    \n",
    "    for weight in class_weight[:-1]:\n",
    "        for maxx in max_features:\n",
    "            for cri in criterion:\n",
    "                for split in splitter:\n",
    "                    dtree.append(DecisionTreeClassifier(class_weight = weight, max_features=maxx, splitter = split, criterion = cri).fit(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_models = svc_models[:1440]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:586: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n"
     ]
    }
   ],
   "source": [
    "for size in test_sizes[3:]:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df[df.columns[:-1]],df[df.columns[-1]], test_size = size)\n",
    "    x_train = np.array(x_train).astype(np.int8)\n",
    "    y_train = np.array(y_train).astype(np.int8)\n",
    "    x_test = np.array(x_test).astype(np.int8)\n",
    "    y_test = np.array(y_test).astype(np.int8)\n",
    "    y_train = np.where(y_train == -1, 0, y_train)\n",
    "    y_test = np.where(y_test == -1, 0, y_test)\n",
    "    ###############################################\n",
    "    for kernel in kernels:\n",
    "        for c in C:\n",
    "            for deg in degree:\n",
    "                for gam in gamma:\n",
    "                    for decision in decision_function_shape:\n",
    "                        svc_models.append(SVC(kernel = kernel, C = c, degree = deg, gamma = gam, decision_function_shape=decision).fit(x_train, y_train))\n",
    "    ###############################################\n",
    "    for loss in losses:\n",
    "        for c in C:\n",
    "            for multi in multi_class:\n",
    "                lsvc.append(LinearSVC(loss = loss, penalty = 'l2', C = c, multi_class = multi).fit(x_train, y_train))\n",
    "    ###############################################\n",
    "    for k in range(1, 10):\n",
    "        for weight in weights:\n",
    "            for algo in algorithm:\n",
    "                for m in metric:\n",
    "                    knn.append(KNeighborsClassifier(n_neighbors=k, weights = weight, algorithm = algo, metric=m).fit(x_train, y_train))\n",
    "    ###############################################\n",
    "    for a in alpha:\n",
    "        for b in binarize:\n",
    "            for fit in fit_prior:\n",
    "                bnbmodels.append(BernoulliNB(alpha = a, binarize = b, fit_prior = fit).fit(x_train, y_train))\n",
    "    ###############################################    \n",
    "    for weight in class_weight:\n",
    "        for maxx in max_features:\n",
    "            for cri in criterion:\n",
    "                for s in warm_start:\n",
    "                    randomforest.append(RandomForestClassifier(warm_start = s, class_weight=weight, max_features = maxx, criterion = cri).fit(x_train, y_train))\n",
    "    ###############################################    \n",
    "    for weight in class_weight[:-1]:\n",
    "        for maxx in max_features:\n",
    "            for cri in criterion:\n",
    "                for split in splitter:\n",
    "                    dtree.append(DecisionTreeClassifier(class_weight = weight, max_features=maxx, splitter = split, criterion = cri).fit(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pred = []\n",
    "lsvc_pred = []\n",
    "knn_pred = []\n",
    "bnb_pred = []\n",
    "random_pred = []\n",
    "dtree_pred = []\n",
    "y_true = []\n",
    "\n",
    "svc_acc = []\n",
    "lsvc_acc = []\n",
    "knn_acc = []\n",
    "bnb_acc = []\n",
    "random_acc = []\n",
    "dtree_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_sizes)):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df[df.columns[:-1]],df[df.columns[-1]], test_size = test_sizes[i])\n",
    "    x_train = np.array(x_train).astype(np.int8)\n",
    "    y_train = np.array(y_train).astype(np.int8)\n",
    "    x_test = np.array(x_test).astype(np.int8)\n",
    "    y_test = np.array(y_test).astype(np.int8)\n",
    "    y_train = np.where(y_train == -1, 0, y_train)\n",
    "    y_test = np.where(y_test == -1, 0, y_test)\n",
    "    y_true.append(y_test)\n",
    "    \n",
    "    for model in svc_models[((i)*480):((i+1)*480)]:\n",
    "        pred = model.predict(x_test)\n",
    "        svc_pred.append(pred)\n",
    "        svc_acc.append(accuracy_score(y_test, pred))\n",
    "        \n",
    "    for model in lsvc[((i)*(len(lsvc)//5)):((i+1)*(len(lsvc)//5))]:\n",
    "        pred = model.predict(x_test)\n",
    "        lsvc_pred.append(pred)\n",
    "        lsvc_acc.append(accuracy_score(y_test, pred))\n",
    "        \n",
    "    for model in knn[((i)*(len(knn)//5)):((i+1)*(len(knn)//5))]:\n",
    "        pred = model.predict(x_test)\n",
    "        knn_pred.append(pred)\n",
    "        knn_acc.append(accuracy_score(y_test, pred))\n",
    "        \n",
    "    for model in bnbmodels[((i)*(len(bnbmodels)//5)):((i+1)*(len(bnbmodels)//5))]:\n",
    "        pred = model.predict(x_test)\n",
    "        bnb_pred.append(pred)\n",
    "        bnb_acc.append(accuracy_score(y_test, pred))\n",
    "        \n",
    "    for model in randomforest[((i)*(len(randomforest)//5)):((i+1)*(len(randomforest)//5))]:\n",
    "        pred = model.predict(x_test)\n",
    "        random_pred.append(pred)\n",
    "        random_acc.append(accuracy_score(y_test, pred))\n",
    "        \n",
    "    for model in dtree[((i)*(len(dtree)//5)):((i+1)*(len(dtree)//5))]:\n",
    "        pred = model.predict(x_test)\n",
    "        dtree_pred.append(pred)\n",
    "        dtree_acc.append(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score #precision_recall_fscore_support(y_test, model5_pred)\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('svc', 1196, 0.9837192474674384),\n",
       " ('lsvc', 10, 0.9397227245328511),\n",
       " ('knn', 657, 0.9891461649782923),\n",
       " ('bnb', 1, 0.918625678119349),\n",
       " ('random', 108, 0.9909551374819102),\n",
       " ('dtree', 81, 0.9876989869753979))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('svc', np.argmax(svc_acc), svc_acc[np.argmax(svc_acc)]), ('lsvc', np.argmax(lsvc_acc), lsvc_acc[np.argmax(lsvc_acc)]), ('knn', np.argmax(knn_acc), knn_acc[np.argmax(knn_acc)]), ('bnb', np.argmax(bnb_acc), bnb_acc[np.argmax(bnb_acc)]), ('random', np.argmax(random_acc), random_acc[np.argmax(random_acc)]), ('dtree', np.argmax(dtree_acc), dtree_acc[np.argmax(dtree_acc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1214   31]\n",
      " [  14 1505]]\n",
      "accuracy :  0.9837192474674384\n",
      "f1 :  0.9852700490998364\n",
      "precision :  0.9798177083333334\n",
      "recall :  0.9907834101382489\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1245\n",
      "           1       0.98      0.99      0.99      1519\n",
      "\n",
      "    accuracy                           0.98      2764\n",
      "   macro avg       0.98      0.98      0.98      2764\n",
      "weighted avg       0.98      0.98      0.98      2764\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_svc = confusion_matrix(y_true[2], svc_pred[1196])\n",
    "print(cm_svc)\n",
    "print(\"accuracy : \", str(accuracy_score(y_true[2], svc_pred[1196])))\n",
    "print(\"f1 : \", str(f1_score(y_true[2], svc_pred[1196])))\n",
    "print(\"precision : \", str(precision_score(y_true[2], svc_pred[1196])))\n",
    "print(\"recall : \", str(recall_score(y_true[2], svc_pred[1196])))\n",
    "print(str(classification_report(y_true[2], svc_pred[1196])))\n",
    "sns.heatmap(cm_svc, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[664  67]\n",
      " [ 33 895]]\n",
      "accuracy :  0.9397227245328511\n",
      "f1 :  0.9470899470899471\n",
      "precision :  0.9303534303534303\n",
      "recall :  0.9644396551724138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93       731\n",
      "           1       0.93      0.96      0.95       928\n",
      "\n",
      "    accuracy                           0.94      1659\n",
      "   macro avg       0.94      0.94      0.94      1659\n",
      "weighted avg       0.94      0.94      0.94      1659\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_lsvc = confusion_matrix(y_true[0], lsvc_pred[10])\n",
    "print(cm_lsvc)\n",
    "print(\"accuracy : \", str(accuracy_score(y_true[0], lsvc_pred[10])))\n",
    "print(\"f1 : \", str(f1_score(y_true[0], lsvc_pred[10])))\n",
    "print(\"precision : \", str(precision_score(y_true[0], lsvc_pred[10])))\n",
    "print(\"recall : \", str(recall_score(y_true[0], lsvc_pred[10])))\n",
    "print(str(classification_report(y_true[0], lsvc_pred[10])))\n",
    "sns.heatmap(cm_lsvc, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1226   19]\n",
      " [  11 1508]]\n",
      "accuracy :  0.9891461649782923\n",
      "f1 :  0.9901510177281682\n",
      "precision :  0.9875573018991487\n",
      "recall :  0.9927583936800527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      1245\n",
      "           1       0.99      0.99      0.99      1519\n",
      "\n",
      "    accuracy                           0.99      2764\n",
      "   macro avg       0.99      0.99      0.99      2764\n",
      "weighted avg       0.99      0.99      0.99      2764\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_knn = confusion_matrix(y_true[2], knn_pred[657])\n",
    "print(cm_knn)\n",
    "print(\"accuracy : \", str(accuracy_score(y_true[2], knn_pred[657])))\n",
    "print(\"f1 : \", str(f1_score(y_true[2], knn_pred[657])))\n",
    "print(\"precision : \", str(precision_score(y_true[2], knn_pred[657])))\n",
    "print(\"recall : \", str(recall_score(y_true[2], knn_pred[657])))\n",
    "print(str(classification_report(y_true[2], knn_pred[657])))\n",
    "sns.heatmap(cm_knn, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[670  61]\n",
      " [ 74 854]]\n",
      "accuracy :  0.918625678119349\n",
      "f1 :  0.9267498643516007\n",
      "precision :  0.9333333333333333\n",
      "recall :  0.9202586206896551\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91       731\n",
      "           1       0.93      0.92      0.93       928\n",
      "\n",
      "    accuracy                           0.92      1659\n",
      "   macro avg       0.92      0.92      0.92      1659\n",
      "weighted avg       0.92      0.92      0.92      1659\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_bnb = confusion_matrix(y_true[0], bnb_pred[1])\n",
    "print(cm_bnb)\n",
    "print(\"accuracy : \", str(accuracy_score(y_true[0], bnb_pred[1])))\n",
    "print(\"f1 : \", str(f1_score(y_true[0], bnb_pred[1])))\n",
    "print(\"precision : \", str(precision_score(y_true[0], bnb_pred[1])))\n",
    "print(\"recall : \", str(recall_score(y_true[0], bnb_pred[1])))\n",
    "print(str(classification_report(y_true[0], bnb_pred[1])))\n",
    "sns.heatmap(cm_bnb, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1229   16]\n",
      " [   9 1510]]\n",
      "accuracy :  0.9909551374819102\n",
      "f1 :  0.9917898193760263\n",
      "precision :  0.9895150720838795\n",
      "recall :  0.9940750493745886\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1245\n",
      "           1       0.99      0.99      0.99      1519\n",
      "\n",
      "    accuracy                           0.99      2764\n",
      "   macro avg       0.99      0.99      0.99      2764\n",
      "weighted avg       0.99      0.99      0.99      2764\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_random = confusion_matrix(y_true[2], random_pred[108])\n",
    "print(cm_random)\n",
    "print(\"accuracy : \", str(accuracy_score(y_true[2], random_pred[108])))\n",
    "print(\"f1 : \", str(f1_score(y_true[2], random_pred[108])))\n",
    "print(\"precision : \", str(precision_score(y_true[2], random_pred[108])))\n",
    "print(\"recall : \", str(recall_score(y_true[2], random_pred[108])))\n",
    "print(str(classification_report(y_true[2], random_pred[108])))\n",
    "sns.heatmap(cm_random, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1229   16]\n",
      " [  18 1501]]\n",
      "accuracy :  0.9876989869753979\n",
      "f1 :  0.9888010540184453\n",
      "precision :  0.989452867501648\n",
      "recall :  0.9881500987491771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1245\n",
      "           1       0.99      0.99      0.99      1519\n",
      "\n",
      "    accuracy                           0.99      2764\n",
      "   macro avg       0.99      0.99      0.99      2764\n",
      "weighted avg       0.99      0.99      0.99      2764\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_dtree = confusion_matrix(y_true[2], dtree_pred[81])\n",
    "print(cm_dtree)\n",
    "print(\"accuracy : \", str(accuracy_score(y_true[2], dtree_pred[81])))\n",
    "print(\"f1 : \", str(f1_score(y_true[2], dtree_pred[81])))\n",
    "print(\"precision : \", str(precision_score(y_true[2], dtree_pred[81])))\n",
    "print(\"recall : \", str(recall_score(y_true[2], dtree_pred[81])))\n",
    "print(str(classification_report(y_true[2], dtree_pred[81])))\n",
    "sns.heatmap(cm_dtree, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(('svc', 1196, 0.9837192474674384),\n",
    " ('lsvc', 10, 0.9397227245328511),\n",
    " ('knn', 657, 0.9891461649782923),\n",
    " ('bnb', 1, 0.918625678119349),\n",
    " ('random', 108, 0.9909551374819102),\n",
    " ('dtree', 81, 0.9876989869753979))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1440-1196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbf 1 2 scale ovo\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for kernel in kernels:\n",
    "    for c in C:\n",
    "        for deg in degree:\n",
    "            for gam in gamma:\n",
    "                for decision in decision_function_shape:\n",
    "                    if i == 244:\n",
    "                        print(kernel, c, deg, gam, decision)\n",
    "                    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squared_hinge 1 ovr\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for loss in losses:\n",
    "        for c in C:\n",
    "            for multi in multi_class:\n",
    "                if i == 10:\n",
    "                    print(loss, c, multi)\n",
    "                i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 uniform brute minkowski\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for k in range(1, 10):\n",
    "    for weight in weights:\n",
    "        for algo in algorithm:\n",
    "            for m in metric:\n",
    "                if i == 207:\n",
    "                    print(k, weight, algo, m)\n",
    "                i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9686893041323421,\n",
       " 0.9634686416402835,\n",
       " 0.9619575594950955,\n",
       " 0.9540425166649918,\n",
       " 0.9528136663795578)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(knn_acc[:288]) / 288, np.sum(knn_acc[288:288*2]) / 288, np.sum(knn_acc[288*2:288*3])/ 288, np.sum(knn_acc[288*3:288*4])/ 288, np.sum(knn_acc[288*4:])/ 288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 False\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for a in alpha:\n",
    "        for b in binarize:\n",
    "            for fit in fit_prior:\n",
    "                if i == 1:\n",
    "                    print(a, b, fit)\n",
    "                i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced_subsample auto gini True\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for weight in class_weight:\n",
    "        for maxx in max_features:\n",
    "            for cri in criterion:\n",
    "                for s in warm_start:\n",
    "                    if i == 36:\n",
    "                        print(weight, maxx, cri, s)\n",
    "                    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None log2 entropy random\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for weight in class_weight[:-1]:\n",
    "        for maxx in max_features:\n",
    "            for cri in criterion:\n",
    "                for split in splitter:\n",
    "                    if i == 15:\n",
    "                        print(weight, maxx, cri, split)\n",
    "                    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.9852700490998364, 0.9470899470899471, 0.9901510177281682, 0.9267498643516007, 0.9917898193760263, 0.9888010540184453]\n",
    "[0.9798177083333334, 0.9303534303534303, 0.9875573018991487, 0.9333333333333333, 0.9895150720838795, 0.989452867501648]\n",
    "[0.9907834101382489, 0.9644396551724138, 0.9927583936800527, 0.9202586206896551, 0.9940750493745886, 0.9881500987491771]\n",
    "\n",
    "svc lsvc knn bnb random dtree "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "svc 0.9837  rbf 1 2 scale ovofor size in test_sizes[3:]:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df[df.columns[:-1]],df[df.columns[-1]], test_size = size)\n",
    "    x_train = np.array(x_train).astype(np.int8)\n",
    "    y_train = np.array(y_train).astype(np.int8)\n",
    "    x_test = np.array(x_test).astype(np.int8)\n",
    "    y_test = np.array(y_test).astype(np.int8)\n",
    "    y_train = np.where(y_train == -1, 0, y_train)\n",
    "    y_test = np.where(y_test == -1, 0, y_test) \n",
    "f1 :  0.9852700490998364\n",
    "precision :  0.9798177083333334\n",
    "recall :  0.9907834101382489\n",
    "\n",
    "lsvc 0.9397  squared_hinge 1 ovr\n",
    "f1 :  0.9470899470899471\n",
    "precision :  0.9303534303534303\n",
    "recall :  0.9644396551724138\n",
    "\n",
    "knn 0.9891  7 uniform brute minkowski\n",
    "f1 :  0.9901510177281682\n",
    "precision :  0.9875573018991487\n",
    "recall :  0.9927583936800527\n",
    "\n",
    "bnb 0.9186  0 0 false\n",
    "f1 :  0.9267498643516007\n",
    "precision :  0.9333333333333333\n",
    "recall :  0.9202586206896551\n",
    "\n",
    "random 0.9909  balanced_subsample auto gini True\n",
    "f1 :  0.9917898193760263\n",
    "precision :  0.9895150720838795\n",
    "recall :  0.9940750493745886\n",
    "\n",
    "\n",
    "dtree 0.9876  None log2 entropy random\n",
    "f1 :  0.9888010540184453\n",
    "precision :  0.989452867501648\n",
    "recall :  0.9881500987491771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaus = []\n",
    "yg = []\n",
    "for size in test_sizes:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df[df.columns[:-1]],df[df.columns[-1]], test_size = size)\n",
    "    x_train = np.array(x_train).astype(np.int8)\n",
    "    y_train = np.array(y_train).astype(np.int8)\n",
    "    x_test = np.array(x_test).astype(np.int8)\n",
    "    y_test = np.array(y_test).astype(np.int8)\n",
    "    y_train = np.where(y_train == -1, 0, y_train)\n",
    "    y_test = np.where(y_test == -1, 0, y_test)\n",
    "    yg.append(y_test)\n",
    "    gaus.append(GaussianNB().fit(x_train, y_train).predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6134366925064599"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(yg[4], gaus[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1751    1]\n",
      " [1495  623]]\n",
      "accuracy :  0.6134366925064599\n",
      "f1 :  0.45441283734500365\n",
      "precision :  0.9983974358974359\n",
      "recall :  0.29414542020774315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70      1752\n",
      "           1       1.00      0.29      0.45      2118\n",
      "\n",
      "    accuracy                           0.61      3870\n",
      "   macro avg       0.77      0.65      0.58      3870\n",
      "weighted avg       0.79      0.61      0.57      3870\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_g = confusion_matrix(yg[4], gaus[4])\n",
    "print(cm_g)\n",
    "print(\"accuracy : \", str(accuracy_score(yg[4], gaus[4])))\n",
    "print(\"f1 : \", str(f1_score(yg[4], gaus[4])))\n",
    "print(\"precision : \", str(precision_score(yg[4], gaus[4])))\n",
    "print(\"recall : \", str(recall_score(yg[4], gaus[4])))\n",
    "print(str(classification_report(yg[4], gaus[4])))\n",
    "sns.heatmap(cm_g, annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
